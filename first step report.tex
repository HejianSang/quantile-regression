\documentclass[]{article}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
%\usepackage{setspace}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}

%opening
\title{Draft: Quantile estimation with using auxiliary information}


\begin{document}

\maketitle

\section{Motivation}
Suppose we have a population, we can always observe the information X. But we can only observe the sample of $Y$ from the population. Our target is to estimate the population quantile of $Y$.

\section{Method}
\subsection{Direct Method}
A very naive method is to use the sample of $Y$. Define $$F_n(y)=\frac{\sum_{i\in A} d_i I\left(y_i\leq y \right) }{\sum_{i\in A}d_i}$$. 
Then we can get the estimated quantile by $$\hat \theta_d=\inf\left\lbrace y:F_n(y)\geq \tau \right\rbrace $$

\subsection{Regression estimator and difference estimate under pseudo empirical log-likelihood function}

We put these two estimators together, because they are asymptotically same. The theorem 1 in the paper shows that. $$\hat \theta_{df}=\hat{\theta_d}+N^{-1}\left\lbrace \sum_{i=1}^{N} q\left(\textbf x_i;\hat\beta_{\tau_0}\right) - \sum_{i\in A} d_i q\left(\textbf x_i;\hat\beta_{\tau_0} \right) \right\rbrace $$

For this estimator, we can extend to multi-calibrations. We can use $\tau_0,\tau_1,...,\tau_m$ to do the calibration together.

\subsection{Using empirical distribution to do calibration}
Define $$F_N(x)=\frac{\sum_{i}^{N}I(x_i \leq x)}{N}$$
$$F_w(x)=\sum_{i\in A} w_i I(x_i \leq x) $$

We want to minimize
\begin{eqnarray}
&& \arg\min_{w} Var\left( \hat\theta_w\right) \approxeq \left(\frac{1}{f(\hat\theta_{\tau})} \right)^2 Var \left(\hat F_w(\theta)\right) \\
&& \text{Given constraints:}\nonumber\\
&& F_N(x)=F_w(x)\nonumber\\
&& \sum_{i\in A} w_i=1\nonumber
\end{eqnarray}
But unfortunately, this constraint is not always holds for any $x\in \mathbb{R}$. So I just use the moments to do the calibration. Because the theorem below:
\begin{theorem}
	(Frechet-Shohat Therom) Suppose ${X_n},n \geq 0$ are random variables. If $\varliminf EX_n^r=\beta_r$ for all $r$ and if all $\beta_r$ are the moments of a unique random variable $X_0$, then $X_n\longrightarrow X_0$ in distribution.
\end{theorem}

So we can use the first four moments to do the calibration. But I don't get the minimized weights.  I first to do the simulation to see if this works. Then we can minimize the $w$ to see we can get the improvement.

\subsection{Ratio estimator}
For any quantile $\tau$, we can get the true population quantile for $X$. So the ratio estimator is possible. Suppose the $\theta_{x,\tau}$ is the quantile of $\tau$ for population $X$.

The ratio estimator is $$ \hat\theta_{ratio}= \theta_{x,\tau}\frac{\hat\theta_{w,y}}{\hat\theta_{w,x}} $$
 But in some cases if the $\hat\theta_{w,x}$ is 0 or closed to 0. That may cause problems.
 We may use the bias-corrected ratio estimator. 
 $$ \hat\theta_{ratio}= \theta_{x,\tau}\frac{\hat\theta_{w,y}\hat\theta_{w,x}+\hat C\left(\hat\theta_{w,y},\hat\theta_{w,x} \right) }{\hat\theta_{w,x}^2+\hat V\left( \hat\theta_{w,x}\right) } $$
 
 The problem is that can we use $$\hat C\left(\hat\theta_{w,y},\hat\theta_{w,x} \right) =??$$
 and $$ \hat  V\left( \hat\theta_{w,x}\right) \approxeq \left(\frac{1}{f(\hat\theta_{\tau})} \right)^2 \hat Var \left(\hat F_w(\theta)\right)$$
 
 \section{A small simulation study}
 The simulation studies are conducted to compare the performance of different estimators. Two finite populations of size $N=1000$ were generated from bivariate normal distribution respectively. The correlation between two variables in the first population is 0.9, then 0.6. For each simulation run , a simple random sample of size $n=100$ was taken.
 
 To compare with each other, we set the direct estimator as the base and define relative efficiency: $$RE_{*}=\frac{MSE_{*}}{MSE_{Direct}}$$.
 
 Note: We use the same $\tau_0=0.5$ to get the difference estimator.
 
 \begin{table}[H]
 	\centering
 	\begin{tabular}{rrrrrr}
 		\hline
 		& $\tau=0.1$ & $\tau=0.3$ & $\tau=0.5$ & $\tau=0.7$ & $\tau=0.9$ \\ 
 		\hline
 		Difference estimator RE & 0.8380 & 0.7203 & 0.7767 & 0.8469 & 0.8599 \\ 
 		ratio estimator RE & 1.2040 & 2.2434 & 7.7272 & 2.1759 & 1.9842 \\ 
 		Moment estimator RE & 0.8392 & 0.7665 & 0.8253 & 0.8852 & 0.9038 \\ 
 		\hline
 	\end{tabular}
 	\caption{RE for different estimators for $\rho=0.6$}
 \end{table}
 
 \begin{table}[H]
 	\centering
 	\begin{tabular}{rrrrrr}
 		\hline
 		& $\tau=0.1$ & $\tau=0.3$ & $\tau=0.5$ & $\tau=0.7$ & $\tau=0.9$ \\ 
 		\hline
 		Difference estimator RE & 0.7419 & 0.5571 & 0.4283 & 0.5278 & 0.7796 \\ 
 		ratio estimator RE & 0.5734 & 0.5104 & 15.6849 & 0.8469 & 0.8919 \\ 
 		Moment estimator RE & 0.7390 & 0.5520 & 0.4329 & 0.5392 & 0.7891 \\ 
 		\hline
 	\end{tabular}
 	\caption{RE for different estimators for $\rho=0.9$}
 \end{table}
 \begin{itemize}
 	\item Both difference estimator and moment estimator do better than direct estimator.
 	\item Ratio estimator is worse than estimator than the direct estimator. Especially at $\tau=0.5$ . That is because the quantile at $\tau=0.5$ is very closed to 0. So bias-corrected ratio estimator may have a better estimation.
 	\item Difference estimator is best in these estimators.
 \end{itemize}
 
 \section{Variance Estimation for difference estimator}
 
In the paper, we have two method to get the variance estimator for quantile estimator.

For method one: I correct two mistakes in the paper. I get the 
\begin{table}[H]
	\begin{tabular}{rrrrrr}
		\hline
		source & $\tau=0.1$ & $\tau=0.3$ & $\tau=0.5$ & $\tau=0.7$ & $\tau=0.9$\\
		\hline
		$\frac{E(\hat V)}{V}-1$ $\rho=0.6$ & 5.84\% & 21.51\% & 34.79824\% & 11.16\% &  10.86\%\\
		$\frac{E(\hat V)}{V}-1$ $\rho=0.9$ & -11.51\% & -8.18\% & 24.20\% & 23.66\% & 0.2718\%\\
		\hline
	\end{tabular}
	\caption{Relative bias for variance estimations}
\end{table}
 
For method two:
 \begin{table}[H]
 	\begin{tabular}{rrrrrr}
 		\hline
 		source & $\tau=0.1$ & $\tau=0.3$ & $\tau=0.5$ & $\tau=0.7$ & $\tau=0.9$\\
 		\hline
 		$\frac{E(\hat V)}{V}-1$ $\rho=0.6$ &344.69\% & 320.10\% & 322.11\% & 303.28\% &  495.60\%\\
 		$\frac{E(\hat V)}{V}-1$ $\rho=0.9$ & 355.76\% & 283.12\% & 315.44\% & 329.97\% & 388.80\%\\
 		\hline
 	\end{tabular}
 	\caption{Relative bias for variance estimations from Woodruff method}
 \end{table}
 
\end{document}
